import requests
from bs4 import BeautifulSoup
import time
import csv

from crawler.worker import app
@app.task()
def crawler_finmind(stock_id):   
    sess = requests.Session()
    sess.cookies.set('over18', '1')  # è§£é”æ¿

    base = 'https://www.ptt.cc'
    url = base + '/bbs/Stock/index.html'

    all_data = []

    for page in range(10):  # æŠ“å–å‰10é 
        print(f'ğŸ” æ­£åœ¨æŠ“å–ç¬¬ {page+1} é : {url}')
        res = sess.get(url)
        soup = BeautifulSoup(res.text, 'html.parser')

        for entry in soup.select('div.r-ent'):
            date = entry.select_one('div.date').get_text(strip=True)
            title_tag = entry.select_one('div.title a')
            if not title_tag:
                continue  # è¢«åˆªé™¤çš„æ–‡ç« 
            title = title_tag.get_text(strip=True)
            link = base + title_tag['href']
            net = entry.select_one('div.nrec').get_text(strip=True)

            try:
                sub = sess.get(link, timeout=5)
                sp2 = BeautifulSoup(sub.text, 'html.parser')
            except:
                continue  # å¿½ç•¥ç„¡æ³•é€£ç·šçš„æ–‡ç« 

            push = boo = arrow = 0
            for p in sp2.select('div.push'):
                tag_span = p.select_one('span.push-tag')
                if not tag_span:
                    continue  # æ²’æœ‰æ‰¾åˆ° push-tagï¼Œè·³é
                tag = tag_span.get_text(strip=True)
                if tag == 'æ¨':
                    push += 1
                elif tag == 'å™“':
                    boo += 1
                elif tag == 'â†’':
                    arrow += 1

            all_data.append({
                'date': date,
                'title': title,
                'push': push,
                'boo': boo,
                'arrow': arrow,
                'net': net,
                'link': link
            })
            print(date, title, f"æ¨:{push}", f"å™“:{boo}", f"ç®­é ­:{arrow}", f"å‡€æ¨:{net}")
            time.sleep(0.05)  # é¿å…è«‹æ±‚éå¿«

        # æ‰¾ä¸Šä¸€é é€£çµ
        paging = soup.select('div.btn-group-paging a')
        prev_url = base + paging[1]['href']
        url = prev_url
        time.sleep(0.5)

    # è¼¸å‡º CSV
    with open('Output/ptt_stock_10_pages.csv', 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=['date', 'title', 'push', 'boo', 'arrow', 'net', 'link'])
        writer.writeheader()
        writer.writerows(all_data)

    print(f"âœ… å®Œæˆï¼Œå…±æŠ“å– {len(all_data)} ç¯‡æ–‡ç« ï¼Œçµæœå·²å„²å­˜åˆ° ptt_stock_10_pages.csv")
